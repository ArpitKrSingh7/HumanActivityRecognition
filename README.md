# üèÉ Human Activity Recognition using LSTM & GRU (PyTorch)

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

Deep Learning-based Human Activity Recognition system using LSTM and GRU networks trained on smartphone inertial sensor data from the UCI HAR Dataset.

---

## üìã Table of Contents

- [Overview](#overview)
- [Activities Recognized](#activities-recognized)
- [Dataset](#dataset)
- [Project Structure](#project-structure)
- [Installation](#installation)
- [Usage](#usage)
- [Model Architecture](#model-architecture)
- [Results](#results)
- [Key Files](#key-files)
- [Future Improvements](#future-improvements)
- [Author](#author)
- [References](#references)

---

## üéØ Overview

This project implements **Human Activity Recognition (HAR)** using deep learning models (LSTM & GRU) to classify human activities based on smartphone accelerometer and gyroscope sensor data.

### Key Features

‚úÖ **LSTM & GRU Networks** for time-series classification  
‚úÖ **98%+ Training Accuracy**, **93% Test Accuracy**  
‚úÖ Complete data preprocessing pipeline  
‚úÖ Model persistence with `scaler.pkl` and `label_encoder.pkl`  
‚úÖ Real-time prediction script (`predict.py`)  
‚úÖ Comprehensive visualizations (confusion matrix, loss/accuracy curves)  
‚úÖ Export functionality for sample data  

---

## üé¨ Activities Recognized

The model can classify **6 different human activities**:

| Activity ID | Activity Name | Description |
|-------------|---------------|-------------|
| 1 | **WALKING** | Walking on flat surface üö∂ |
| 2 | **WALKING_UPSTAIRS** | Climbing up stairs ‚¨ÜÔ∏è |
| 3 | **WALKING_DOWNSTAIRS** | Descending stairs ‚¨áÔ∏è |
| 4 | **SITTING** | Sitting position ü™ë |
| 5 | **STANDING** | Standing still üßç |
| 6 | **LAYING** | Lying down position üõå |

---

## üìä Dataset

### UCI Human Activity Recognition Dataset

- **Source**: [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/human+activity+recognition+using+smartphones)
- **Participants**: 30 volunteers (19-48 years)
- **Sensors**: Accelerometer & Gyroscope (3-axial: X, Y, Z)
- **Sampling Rate**: 50 Hz
- **Window Size**: 128 readings per window (2.56 seconds)
- **Train/Test Split**: 70% / 30%

#### Sensor Signals
- **Accelerometer**: `body_acc_x`, `body_acc_y`, `body_acc_z`, `total_acc_x/y/z`
- **Gyroscope**: `body_gyro_x`, `body_gyro_y`, `body_gyro_z`
- **Total Features**: 9 raw signals per window

---

## üìÅ Project Structure
```
HumanActivityRecognition/
‚îÇ
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ UCIHARDataset/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ activity_labels.txt        # Activity ID to name mapping
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ features.txt               # Feature names
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ features_info.txt          # Feature descriptions
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ README.txt                 # Dataset documentation
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ train/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Inertial Signals/      # Raw sensor data (train)
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ X_train.txt            # Training features
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ y_train.txt            # Training labels
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ subject_train.txt      # Subject IDs
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ test/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ Inertial Signals/      # Raw sensor data (test)
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ X_test.txt             # Test features
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ y_test.txt             # Test labels
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ subject_test.txt       # Subject IDs
‚îÇ   ‚îî‚îÄ‚îÄ __MACOSX/                      # Mac metadata (can be ignored)
‚îÇ
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îî‚îÄ‚îÄ HumanActivityDetection.ipynb   # Main Jupyter notebook
‚îÇ
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îî‚îÄ‚îÄ best_model/
‚îÇ       ‚îî‚îÄ‚îÄ best_model.pth             # Saved model weights
‚îÇ
‚îú‚îÄ‚îÄ results/
‚îÇ   ‚îú‚îÄ‚îÄ Confusion_Matrix_test_data.png # Test confusion matrix
‚îÇ   ‚îú‚îÄ‚îÄ confusion_matrix_val_data.png  # Validation confusion matrix
‚îÇ   ‚îú‚îÄ‚îÄ train_acc.png                  # Training accuracy plot
‚îÇ   ‚îú‚îÄ‚îÄ train_loss.png                 # Training loss plot
‚îÇ   ‚îú‚îÄ‚îÄ val_acc.png                    # Validation accuracy plot
‚îÇ   ‚îú‚îÄ‚îÄ val_loss.png                   # Validation loss plot
‚îÇ   ‚îî‚îÄ‚îÄ metrics.txt                    # Detailed performance metrics
‚îÇ
‚îú‚îÄ‚îÄ predict.py                         # Prediction script
‚îú‚îÄ‚îÄ export_test_sample.py              # Export sample data for testing
‚îú‚îÄ‚îÄ scaler.pkl                         # Saved StandardScaler
‚îú‚îÄ‚îÄ label_encoder.pkl                  # Saved LabelEncoder
‚îú‚îÄ‚îÄ sample_data.csv                    # Sample sensor data
‚îú‚îÄ‚îÄ sample_from_dataset.csv            # Sample from UCI dataset
‚îú‚îÄ‚îÄ requirements.txt                   # Python dependencies
‚îî‚îÄ‚îÄ README.md                          # Project documentation
```

---

## üöÄ Installation

### Prerequisites
- Python 3.8 or higher
- pip package manager
- Virtual environment (recommended)

### Step 1: Clone the Repository
```bash
git clone https://github.com/ArpitKrSingh7/HumanActivityRecognition.git
cd HumanActivityRecognition
```

### Step 2: Create Virtual Environment
```bash
python -m venv venv

# On Linux/Mac:
source venv/bin/activate

# On Windows:
venv\Scripts\activate
```

### Step 3: Install Dependencies
```bash
pip install -r requirements.txt
```

**requirements.txt**:
```txt
torch>=2.0.0
torchvision>=0.15.0
pandas>=1.5.0
numpy>=1.24.0
matplotlib>=3.7.0
seaborn>=0.12.0
scikit-learn>=1.3.0
jupyter>=1.0.0
tqdm>=4.65.0
joblib>=1.3.0
```

---

## üíª Usage

### Training the Model

Open and run the Jupyter notebook:
```bash
jupyter notebook notebooks/HumanActivityDetection.ipynb
```

The notebook covers:
1. **Data Loading** - Loading UCI HAR dataset
2. **Exploratory Data Analysis (EDA)** - Visualizing sensor signals and activity distribution
3. **Data Preprocessing** - Normalization and encoding
4. **Model Building** - LSTM and GRU architectures
5. **Training** - Model training with validation
6. **Evaluation** - Performance metrics and confusion matrix
7. **Model Saving** - Saving best model and preprocessors

### Making Predictions

Use the `predict.py` script for real-time predictions:
```bash
python predict.py --input sample_data.csv
```

**Example Usage in Python:**
```python
import torch
import pandas as pd
import joblib
from predict import load_model, predict_activity

# Load model and preprocessors
model = load_model('models/best_model/best_model.pth')
scaler = joblib.load('scaler.pkl')
label_encoder = joblib.load('label_encoder.pkl')

# Load sample data
data = pd.read_csv('sample_data.csv')

# Make prediction
prediction = predict_activity(model, data, scaler, label_encoder)
print(f"Predicted Activity: {prediction}")
```

### Exporting Test Samples

Generate sample data for testing:
```bash
python export_test_sample.py
```

This creates `sample_from_dataset.csv` with sensor readings from the test set.

---

## üß† Model Architecture

### LSTM Model
```
Input: (batch_size, 128, 9)  # 128 timesteps, 9 features
    ‚Üì
LSTM Layer 1: 64 hidden units + Dropout(0.3)
    ‚Üì
LSTM Layer 2: 64 hidden units + Dropout(0.3)
    ‚Üì
Fully Connected 1: 128 neurons + ReLU + Dropout(0.3)
    ‚Üì
Fully Connected 2: 6 neurons (output classes)
    ‚Üì
Output: (batch_size, 6)
```

### GRU Model
```
Input: (batch_size, 128, 9)  # 128 timesteps, 9 features
    ‚Üì
GRU Layer 1: 64 hidden units + Dropout(0.3)
    ‚Üì
GRU Layer 2: 64 hidden units + Dropout(0.3)
    ‚Üì
Fully Connected 1: 128 neurons + ReLU + Dropout(0.3)
    ‚Üì
Fully Connected 2: 6 neurons (output classes)
    ‚Üì
Output: (batch_size, 6)
```

### Hyperparameters

| Parameter | Value |
|-----------|-------|
| Input Features | 9 (3 acc + 3 gyro + 3 total_acc) |
| Hidden Units | 64 |
| LSTM/GRU Layers | 2 |
| FC Layer Size | 128 |
| Output Classes | 6 |
| Dropout Rate | 0.3 |
| Learning Rate | 0.001 |
| Batch Size | 32 |
| Epochs | 50 |
| Optimizer | Adam |
| Loss Function | CrossEntropyLoss |

---

## üìà Results

### Performance Metrics

| Metric | Training | Test |
|--------|----------|------|
| **Accuracy** | **98.5%** | **93.0%** |
| **Loss** | 0.12 | 0.25 |
| **Precision** | 0.985 | 0.930 |
| **Recall** | 0.985 | 0.930 |
| **F1-Score** | 0.985 | 0.930 |

### Training Curves

The training process shows consistent improvement:

**Training Loss**  
![Training Loss](results/train_loss.png)

**Training Accuracy**  
![Training Accuracy](results/train_acc.png)

**Validation Loss**  
![Validation Loss](results/val_loss.png)

**Validation Accuracy**  
![Validation Accuracy](results/val_acc.png)

### Confusion Matrix

**Validation Data Confusion Matrix**  
![Confusion Matrix - Validation](results/confusion_matrix_val_data.png)

**Test Data Confusion Matrix**  
![Confusion Matrix - Test](results/Confusion_Matrix_test_data.png)

### Per-Class Performance

Detailed metrics are available in `results/metrics.txt`

**Best Performing Activities:**
- Laying: ~99% accuracy (easiest to classify)
- Standing: ~97% accuracy
- Walking: ~95% accuracy

**Challenging Activities:**
- Walking Upstairs vs Walking Downstairs: Some confusion due to similar motion patterns
- Sitting vs Standing: Occasional misclassification

---

## üîë Key Files

### Model Files

- **`models/best_model/best_model.pth`**: Saved PyTorch model weights
- **`scaler.pkl`**: StandardScaler for feature normalization
- **`label_encoder.pkl`**: LabelEncoder for activity labels

### Scripts

- **`predict.py`**: Standalone prediction script
  - Loads trained model
  - Preprocesses input data
  - Returns predicted activity

- **`export_test_sample.py`**: Export utility
  - Extracts sample sequences from test set
  - Saves as CSV for demo purposes

### Data Files

- **`sample_data.csv`**: Example sensor data for testing
- **`sample_from_dataset.csv`**: Real samples from UCI dataset

### Results

All visualizations and metrics are saved in the `results/` directory:
- Confusion matrices (validation & test)
- Training/validation loss curves
- Training/validation accuracy curves
- Detailed performance metrics

---

## üë®‚Äçüíª Author

**Arpit Kumar Singh**

- üîó LinkedIn: [My Linkedin](https://www.linkedin.com/in/arpit-kumar-singh-aks100606)
- üíª GitHub: [@ArpitKrSingh7](https://github.com/ArpitKrSingh7)
- üìß Email: [Arpitkumarsingh9470@gmail.com]

---

## üìö References

### Dataset
1. **UCI HAR Dataset**: Davide Anguita, Alessandro Ghio, Luca Oneto, Xavier Parra and Jorge L. Reyes-Ortiz. *Human Activity Recognition Using Smartphones Dataset.* UCI Machine Learning Repository, 2012.

### Research Papers
1. Anguita, D., Ghio, A., Oneto, L., Parra, X., & Reyes-Ortiz, J. L. (2013). *A public domain dataset for human activity recognition using smartphones.* ESANN 2013.

2. Hochreiter, S., & Schmidhuber, J. (1997). *Long short-term memory.* Neural computation, 9(8), 1735-1780.

3. Cho, K., et al. (2014). *Learning phrase representations using RNN encoder-decoder for statistical machine translation.* EMNLP 2014.

### Documentation
- [PyTorch Documentation](https://pytorch.org/docs/)
- [Scikit-learn Documentation](https://scikit-learn.org/)
- [UCI ML Repository](https://archive.ics.uci.edu/ml/)

---

## üìÑ License

This project is licensed under the MIT License - feel free to use it for educational and research purposes.

---

## üìû Support

If you have any questions or suggestions:

- üêõ **Issues**: [GitHub Issues](https://github.com/ArpitKrSingh7/HumanActivityRecognition/issues)
- üí¨ **Discussions**: [GitHub Discussions](https://github.com/ArpitKrSingh7/HumanActivityRecognition/discussions)
- üìß **Contact**: arpitkumarsingh9470@gmail.com

---

<div align="center">

*Human Activity Recognition using Deep Learning*

</div>
